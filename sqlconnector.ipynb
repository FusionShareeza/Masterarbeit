{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow/Keras: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "#data preprocessing\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "#ml stuff\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras \n",
    "import tensorflow as tf\n",
    "print('Tensorflow/Keras: %s' % keras.__version__)\n",
    "from keras.models import Sequential \n",
    "from keras import Input \n",
    "from keras.layers import Dense\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#utils/visualization\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dill as pickle\n",
    "from ydata_profiling import ProfileReport\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.express as px\n",
    "import itertools\n",
    "import functools\n",
    "import operator \n",
    "from more_itertools import flatten\n",
    "from collections import Counter\n",
    "#config vars\n",
    "pd.options.display.max_columns=1000\n",
    "pd.options.display.max_rows = 10000\n",
    "pd.options.display.max_seq_items = 100\n",
    "verbose=1\n",
    "\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import sqlalchemy as sa\n",
    "import urllib\n",
    "from sqlalchemy import text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Database and get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lete\\AppData\\Local\\Temp\\ipykernel_10816\\2088681304.py:16: UserWarning:\n",
      "\n",
      "pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OLD and slower\n",
    "def connect_to_db(connection_string,\n",
    "                    database,\n",
    "                    driver = 'SQL Server Native Client 11.0',\n",
    "                    user = 'CCAdmin',\n",
    "                    password = 'Miw6RjnTGmPHLYF9mG1o'\n",
    "):\n",
    "    connection = pyodbc.connect(\"Driver={\"+driver+\"};\"\n",
    "                        \"Server=\"+connection_string+\";\"\n",
    "                        \"Database=\"+database+\";\"\n",
    "                        \"uid=\"+user+\";pwd=\"+password+\"\")\n",
    "    return connection\n",
    "\n",
    "def get_table_data(table_name, connection):\n",
    "    query = \"SELECT * FROM {}\".format(table_name)\n",
    "    df = pd.read_sql_query(query, connection)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_table_cclogattributes = get_table_data('CCLogAttributes', connect_to_db(connection_string= 'classconprocessingger.database.windows.net', database = 'T_'+tenant+''))\n",
    "df_table_cclogattributes = df_table_cclogattributes.drop(['Zone','Attribute_DataType','LogTimeTicks'], axis=1)\n",
    "\n",
    "df_table_cclogattributes = df_table_cclogattributes.replace('\\n',' ', regex=True)\n",
    "df_table_cclogattributes = df_table_cclogattributes.replace('\\r',' ', regex=True)\n",
    "df_table_cclogattributes.to_csv('data/cclogattributes_T_'+tenant+'.csv', index=False, header= True, encoding='utf-8')#iso-8859-15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenant = '0001ai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_db_better(connection_string,\n",
    "                    database,\n",
    "                    driver = 'SQL Server Native Client 11.0',\n",
    "                    user = 'CCAdmin',\n",
    "                    password = 'Miw6RjnTGmPHLYF9mG1o'\n",
    "):\n",
    "    odbc_str = 'DRIVER='+driver+';SERVER='+connection_string+';PORT=1433;UID='+user+';DATABASE='+ database + ';PWD='+ password\n",
    "    connect_str = 'mssql+pyodbc:///?odbc_connect=' + urllib.parse.quote_plus(odbc_str)\n",
    "\n",
    "    return connect_str\n",
    "\n",
    "\n",
    "def get_table_data_CCLOG(table_name, connect_str, startdate, enddate):\n",
    "    engine = create_engine(connect_str)\n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(text(\"SELECT * FROM [dbo].[\"+table_name+\"] where LogTime >= \"+startdate+\" and LogTime <= \"+enddate+\"\"), conn)\n",
    "        return df\n",
    "\n",
    "def get_table_data_ALL(table_name, connect_str):\n",
    "    engine = create_engine(connect_str)\n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(text(\"SELECT * FROM [dbo].[\"+table_name+\"]\"), conn)\n",
    "        return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()\n",
    "#ProfileReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "asdf"
    ]
   },
   "outputs": [],
   "source": [
    "dist = df['Delta'].value_counts()\n",
    "trace = go.Pie(values=(np.array(dist)),labels=dist.index,  pull=[0, 0])\n",
    "layout = go.Layout(title='Delta Distribution')\n",
    "data = [trace]\n",
    "fig = go.Figure(trace,layout)\n",
    "fig.update_traces(marker=dict(line=dict(color='#000000', width=0.5)), textinfo='value+percent', insidetextorientation='auto')\n",
    "#sfig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = df['Type'].value_counts()\n",
    "trace = go.Pie(values=(np.array(dist)),labels=dist.index,  pull=[0, 0.4,0.2,0.1])\n",
    "layout = go.Layout(title='Type Distribution')\n",
    "data = [trace]\n",
    "fig = go.Figure(trace,layout)\n",
    "fig.update_traces(marker=dict(line=dict(color='#000000', width=0.5)), textinfo='value+percent', insidetextorientation='auto')\n",
    "#sfig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sollerkennungswerte bestimmen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sollwerte(key, df):\n",
    "    df_ordernum = df.loc[(df['Attribute_Name'] == 'OrderNum')] #&(df['Delta'] == False)\n",
    "    dist = df_ordernum['Delta'].value_counts(normalize=True)\n",
    "    print('Kreditor: '+key+'')\n",
    "    \n",
    "    try:\n",
    "        score = 1 - dist.loc[True]\n",
    "    except:\n",
    "        score = dist.loc[False]\n",
    "\n",
    "    print('q(OrderNum): '+str(score)+'') \n",
    "\n",
    "\n",
    "    df_ordernum_correct = df.loc[(df['Attribute_Name'] == 'OrderNum') &(df['Delta'] == False)] \n",
    "    documentid_list = df_ordernum_correct[['DocumentID']].values.tolist()\n",
    "    merged = list(itertools.chain.from_iterable(documentid_list))\n",
    "    denonimator_ordernum = len(documentid_list)\n",
    "    \n",
    "\n",
    "    df_debitor_ordernum = df.loc[(df['Attribute_Name'] == 'OrderNum')|(df['Attribute_Name'] == 'DEBITOR_NUM')|(df['Attribute_Name'] == 'VENDOR_NUM')]\n",
    "    #print(df_debitor_ordernum)\n",
    "    #df_debitor_ordernum.to_csv('data/debitor_preprocessing.csv', index=False, header= True, encoding='iso-8859-15')\n",
    "    \n",
    "\n",
    "\n",
    "    counter_do=0\n",
    "    counter_vo=0\n",
    "    for entry_1 in merged:\n",
    "        speicher_entry_new = df_debitor_ordernum[df_debitor_ordernum.DocumentID == ''.join(str(entry_1))]\n",
    "        if ((speicher_entry_new.Attribute_Name == 'OrderNum') & (speicher_entry_new.Delta == False)).any(): \n",
    "            if ((speicher_entry_new.Attribute_Name == 'DEBITOR_NUM')&(speicher_entry_new.Delta == False)).any():\n",
    "                    counter_do += 1\n",
    "            if ((speicher_entry_new.Attribute_Name == 'VENDOR_NUM')&(speicher_entry_new.Delta == False)).any():\n",
    "                    counter_vo += 1\n",
    "    try:\n",
    "        print('q(Mandant|OrderNum): '+str(counter_do/denonimator_ordernum)+'')\n",
    "        print('q(Vendor|OrderNum): '+str(counter_vo/denonimator_ordernum)+'')\n",
    "    except:\n",
    "         print('error')\n",
    "\n",
    "    counter_vno=0\n",
    "\n",
    "    df_order_delta = df.loc[(df['Attribute_Name'] == 'OrderNum')&(df['Delta'] == True)]\n",
    "    df_order_delta_documentid = df_order_delta[['DocumentID']]\n",
    "    order_delta_list = df_order_delta_documentid.values.tolist()\n",
    "    order_delta_list_merged = list(itertools.chain.from_iterable(order_delta_list))\n",
    "    denominator_order_delta_num = len(order_delta_list_merged)\n",
    "\n",
    "\n",
    "    df_invoicenumber_order_delta = df.loc[(df['Attribute_Name'] == 'DEBITOR_NUM')|(df['Attribute_Name'] == 'OrderNum')]\n",
    "    for entry_order_delta_invoicenumber in order_delta_list_merged:\n",
    "        speicher_entry_order_delta = df_invoicenumber_order_delta[df_invoicenumber_order_delta.DocumentID == ''.join(str(entry_order_delta_invoicenumber))]\n",
    "        if ((speicher_entry_order_delta.Attribute_Name == 'OrderNum') & (speicher_entry_order_delta.Delta == True)).any(): \n",
    "            if ((speicher_entry_order_delta.Attribute_Name == 'DEBITOR_NUM') & (speicher_entry_order_delta.Delta == False)).any():\n",
    "                    counter_vno += 1\n",
    "\n",
    "    if(counter_vno == 0):\n",
    "        print('Da keine falsche Ordernum existiert ist Mandant|!Ordernum 0')\n",
    "    else:\n",
    "        print('q(Mandant|!OrderNum): '+str(counter_vno/denominator_order_delta_num)+'')\n",
    "        \n",
    "    df_vendor_nodelta = df.loc[(df['Attribute_Name'] == 'VENDOR_NUM')&(df['Delta'] == False)]\n",
    "    df_vendor_documentid = df_vendor_nodelta[['DocumentID']]\n",
    "    vendor_list = df_vendor_documentid.values.tolist()\n",
    "    vendor_list_merged = list(itertools.chain.from_iterable(vendor_list))\n",
    "    denominator_vendor_num = len(vendor_list)\n",
    "\n",
    "    df_invoicenumber_vendor = df.loc[(df['Attribute_Name'] == 'InvoiceNumber')|(df['Attribute_Name'] == 'VENDOR_NUM')|(df['Attribute_Name'] == 'InvoiceDate')|(df['Attribute_Name'] == 'InvoiceDate')|(df['Attribute_Name'] == 'GrossAmount')]\n",
    "\n",
    "    counter_inv=0\n",
    "    counter_idv=0\n",
    "    counter_gv=0\n",
    "\n",
    "    for entry_vendor_invoicenumber in vendor_list_merged:\n",
    "        speicher_entry_vendor = df_invoicenumber_vendor[df_invoicenumber_vendor.DocumentID == ''.join(str(entry_vendor_invoicenumber))]\n",
    "        if ((speicher_entry_vendor.Attribute_Name == 'VENDOR_NUM') & (speicher_entry_vendor.Delta == False)).any(): \n",
    "            if ((speicher_entry_vendor.Attribute_Name == 'InvoiceNumber') & (speicher_entry_vendor.Delta == False)).any():\n",
    "                    counter_inv += 1\n",
    "            if ((speicher_entry_vendor.Attribute_Name == 'InvoiceDate') & (speicher_entry_vendor.Delta == False)).any():\n",
    "                    counter_idv += 1\n",
    "            if ((speicher_entry_vendor.Attribute_Name == 'GrossAmount') & (speicher_entry_vendor.Delta == False)).any():\n",
    "                    counter_gv += 1\n",
    "\n",
    "    try:\n",
    "        print('q(InvoiceNumber|Vendor): '+str(counter_inv/denominator_vendor_num)+'')\n",
    "        print('q(InvoiceDate|Vendor): '+str(counter_idv/denominator_vendor_num)+'')\n",
    "        print('q(Gross|Vendor): '+str(counter_gv/denominator_vendor_num)+'')\n",
    "    except:\n",
    "        print('error')\n",
    "    counter_gnv=0\n",
    "\n",
    "    df_vendor_delta = df.loc[(df['Attribute_Name'] == 'VENDOR_NUM')&(df['Delta'] == True)]\n",
    "    df_vendor_delta_documentid = df_vendor_delta[['DocumentID']]\n",
    "    vendor_delta_list = df_vendor_delta_documentid.values.tolist()\n",
    "    vendor_delta_list_merged = list(itertools.chain.from_iterable(vendor_delta_list))\n",
    "    denominator_vendor_delta_num = len(vendor_delta_list_merged)\n",
    "\n",
    "\n",
    "    df_invoicenumber_vendor_delta = df.loc[(df['Attribute_Name'] == 'GrossAmount')|(df['Attribute_Name'] == 'VENDOR_NUM')]\n",
    "    for entry_vendor_delta_invoicenumber in vendor_delta_list_merged:\n",
    "        speicher_entry_vendor_delta = df_invoicenumber_vendor_delta[df_invoicenumber_vendor_delta.DocumentID == ''.join(str(entry_vendor_delta_invoicenumber))]\n",
    "        if ((speicher_entry_vendor_delta.Attribute_Name == 'VENDOR_NUM') & (speicher_entry_vendor_delta.Delta == True)).any(): \n",
    "            if ((speicher_entry_vendor_delta.Attribute_Name == 'GrossAmount') & (speicher_entry_vendor_delta.Delta == False)).any():\n",
    "                    counter_gnv += 1\n",
    "\n",
    "    try:\n",
    "        print('q(Gross|!Vendor): '+str(counter_gnv/denominator_vendor_delta_num)+'')\n",
    "    except:\n",
    "        print('errror')\n",
    "\n",
    "    counter_vom=0\n",
    "\n",
    "    df_order_mandant_delta = df.loc[(df['Attribute_Name'] == 'OrderNum')&(df['Delta'] == True)]\n",
    "    df_order_delta_new = df_order_mandant_delta.loc[(df_order_mandant_delta['Attribute_Name'] == 'Mandant')&(df['Delta'] == False)]\n",
    "    df_order_delta_mandant_documentid = df_order_delta_new[['DocumentID']]\n",
    "    order_delta_mandant_list = df_order_delta_mandant_documentid.values.tolist()\n",
    "    order_delta_mandant_list_merged = list(itertools.chain.from_iterable(order_delta_mandant_list))\n",
    "    denominator_order_delta_mandant_num = len(order_delta_mandant_list_merged)\n",
    "\n",
    "\n",
    "    df_invoicenumber_order_delta_mandant = df.loc[(df['Attribute_Name'] == 'VENDOR_NUM')|(df['Attribute_Name'] == 'OrderNum')|(df['Attribute_Name'] == 'DEBITOR_NUM')]\n",
    "    for entry_order_delta_mandant in order_delta_mandant_list_merged:\n",
    "        speicher_entry_order_delta_mandant = df_invoicenumber_order_delta_mandant[df_invoicenumber_order_delta_mandant.DocumentID == ''.join(str(entry_order_delta_mandant))]\n",
    "        if (((speicher_entry_order_delta_mandant.Attribute_Name == 'OrderNum') & (speicher_entry_order_delta_mandant.Delta == True))&((speicher_entry_order_delta_mandant.Attribute_Name == 'DEBITOR_NUM') & (speicher_entry_order_delta_mandant.Delta == False))).any(): \n",
    "            if ((speicher_entry_order_delta_mandant.Attribute_Name == 'VENDOR_NUM') & (speicher_entry_order_delta_mandant.Delta == False)).any():\n",
    "                    counter_vom += 1\n",
    "\n",
    "    if((counter_vom == 0) | (denominator_order_delta_mandant_num == 0 & counter_vom == 0)):\n",
    "        print('Nulldivision q(Vendor| !OrderNum & Mandant) ungültig')\n",
    "    else:\n",
    "        print('q(Vendor| !OrderNum & Mandant): '+str(counter_vom/denominator_order_delta_mandant_num)+'')\n",
    "\n",
    "\n",
    "    counter_vnom=0\n",
    "\n",
    "    df_order_mandant_delta = df.loc[(df['Attribute_Name'] == 'OrderNum')&(df['Delta'] == True)]\n",
    "    df_order_delta_new = df_order_mandant_delta.loc[(df_order_mandant_delta['Attribute_Name'] == 'Mandant')&(df['Delta'] == True)]\n",
    "    df_order_delta_mandant_documentid = df_order_delta_new[['DocumentID']]\n",
    "    order_delta_mandant_list = df_order_delta_mandant_documentid.values.tolist()\n",
    "    order_delta_mandant_list_merged = list(itertools.chain.from_iterable(order_delta_mandant_list))\n",
    "    denominator_order_ndelta_mandant_num = len(order_delta_mandant_list_merged)\n",
    "\n",
    "\n",
    "    df_invoicenumber_order_delta_mandant = df.loc[(df['Attribute_Name'] == 'VENDOR_NUM')|(df['Attribute_Name'] == 'OrderNum')|(df['Attribute_Name'] == 'DEBITOR_NUM')]\n",
    "    for entry_order_delta_mandant in order_delta_mandant_list_merged:\n",
    "        speicher_entry_order_delta_mandant = df_invoicenumber_order_delta_mandant[df_invoicenumber_order_delta_mandant.DocumentID == ''.join(str(entry_order_delta_mandant))]\n",
    "        if (((speicher_entry_order_delta_mandant.Attribute_Name == 'OrderNum') & (speicher_entry_order_delta_mandant.Delta == True))&((speicher_entry_order_delta_mandant.Attribute_Name == 'DEBITOR_NUM') & (speicher_entry_order_delta_mandant.Delta == True))).any(): \n",
    "            if ((speicher_entry_order_delta_mandant.Attribute_Name == 'VENDOR_NUM') & (speicher_entry_order_delta_mandant.Delta == False)).any():\n",
    "                    counter_vnom += 1\n",
    "\n",
    "    if((counter_vnom == 0) | (denominator_order_ndelta_mandant_num == 0)):\n",
    "        print('Nulldivision q(Vendor| !OrderNum & !Mandant) ungültig')\n",
    "    else:\n",
    "        print('q(Vendor| !OrderNum & !Mandant): '+str(counter_vnom/denominator_order_ndelta_mandant_num)+'')\n",
    "    \n",
    "    print('--------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nach Kreditoren sortieren\n",
    "#Nach Zeit sortieren \n",
    "def split_datframe_into_whatever(splitkey, df_new):\n",
    "    df_split = df_new.loc[(df_new['Attribute_Name'] == splitkey)] \n",
    "\n",
    "    def split_dataframe(df_debitornum, column):\n",
    "        split_dfs = {}\n",
    "        for value in df_debitornum[column].unique():\n",
    "            split_dfs[value] = df_debitornum[df_debitornum[column] == value][['DocumentID']]   \n",
    "        return split_dfs\n",
    "\n",
    "    dc_split_attribute_after = split_dataframe(df_split, 'Attribute_After')\n",
    "\n",
    "\n",
    "    def find_corresponding_values(dict_of_dataframes, column_to_match, large_dataframe):\n",
    "        result_dict = {}\n",
    "        for key, df in dict_of_dataframes.items():\n",
    "            temp_df = large_dataframe[large_dataframe[column_to_match].isin(df[column_to_match])]\n",
    "            result_dict[key] = temp_df\n",
    "        return result_dict\n",
    "\n",
    "    dc_sorted_df = find_corresponding_values(dc_split_attribute_after, \"DocumentID\", df_new)\n",
    "    return dc_sorted_df\n",
    "    #dc_sorted_df_by_creditor[\"\"].to_csv('data/codia.csv', index=False, header= True, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_by_time(df, start_date, end_date):\n",
    "    # Filtern des DataFrames nach dem Zeitfenster\n",
    "    filtered_df = df[(df['LogTime'] >= start_date) & (df['LogTime'] <= end_date)]  \n",
    "    df_new = filtered_df.drop(['LogTime'], axis =1)\n",
    "    # Rückgabe des gefilterten DataFrames\n",
    "    df_new.to_csv('data/cclogattributes_T_'+tenant+'_reduced.csv', index=False, header= True, encoding='utf-8')#iso-8859-15\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_values(df, value1, bool1, value2, bool2): \n",
    "    df_ordernum_correct = df.loc[(df['Attribute_Name'] == value1) &(df['Delta'] == bool1)] \n",
    "    documentid_list = df_ordernum_correct[['DocumentID']].values.tolist()\n",
    "    merged = list(itertools.chain.from_iterable(documentid_list))\n",
    "    denonimator_ordernum = len(documentid_list)\n",
    "    \n",
    "    df_debitor_ordernum = df.loc[(df['Attribute_Name'] == value1)|(df['Attribute_Name'] == value2)]\n",
    "\n",
    "    counter_do=0\n",
    "    for entry_1 in merged:\n",
    "        speicher_entry_new = df_debitor_ordernum[df_debitor_ordernum.DocumentID == ''.join(str(entry_1))]\n",
    "        if ((speicher_entry_new.Attribute_Name == value1) & (speicher_entry_new.Delta == bool1)).any(): \n",
    "            if ((speicher_entry_new.Attribute_Name == value2)&(speicher_entry_new.Delta == bool2)).any():\n",
    "                    counter_do += 1\n",
    "                    x = speicher_entry_new[\"DocumentID\"]\n",
    "\n",
    "    try:\n",
    "        return counter_do/denonimator_ordernum , x\n",
    "        #return ([counter_do/denonimator_ordernum,counter_do,denonimator_ordernum])\n",
    "    except:\n",
    "         return pd.NA, pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_values2(df, items): \n",
    "    df_ordernum_correct = df.loc[(df['Attribute_Name'] == items[0]) &(df['Delta'] == items[1])] \n",
    "    documentid_list = df_ordernum_correct[['DocumentID']].values.tolist()\n",
    "    merged = list(itertools.chain.from_iterable(documentid_list))\n",
    "    denonimator_ordernum = len(documentid_list)\n",
    "    \n",
    "    df_debitor_ordernum = df.loc[(df['Attribute_Name'] == items[0])|(df['Attribute_Name'] == items[2])]\n",
    "\n",
    "    counter_do=0\n",
    "    for entry_1 in merged:\n",
    "        speicher_entry_new = df_debitor_ordernum[df_debitor_ordernum.DocumentID == ''.join(str(entry_1))]\n",
    "        if ((speicher_entry_new.Attribute_Name == items[0]) & (speicher_entry_new.Delta == items[1])).any(): \n",
    "            if ((speicher_entry_new.Attribute_Name == items[2])&(speicher_entry_new.Delta == False)).any():\n",
    "                    counter_do += 1\n",
    "\n",
    "    try:\n",
    "        #return counter_do/denonimator_ordernum\n",
    "        return counter_do/denonimator_ordernum,denonimator_ordernum\n",
    "    except:\n",
    "         return pd.NA, pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_values_two_booleans(df, value1, bool1, value2, bool2, value3):    \n",
    "    counter_vom=0\n",
    "\n",
    "    df_order_mandant_delta = df.loc[(df['Attribute_Name'] == value1)&(df['Delta'] == bool1)]\n",
    "    df_order_delta_new = df_order_mandant_delta.loc[(df_order_mandant_delta['Attribute_Name'] == value2)&(df['Delta'] == bool2)]\n",
    "    df_order_delta_mandant_documentid = df_order_delta_new[['DocumentID']]\n",
    "    order_delta_mandant_list = df_order_delta_mandant_documentid.values.tolist()\n",
    "    order_delta_mandant_list_merged = list(itertools.chain.from_iterable(order_delta_mandant_list))\n",
    "    denominator_order_delta_mandant_num = len(order_delta_mandant_list_merged)\n",
    "\n",
    "\n",
    "    df_invoicenumber_order_delta_mandant = df.loc[(df['Attribute_Name'] == value1)|(df['Attribute_Name'] == value2)|(df['Attribute_Name'] == value3)]\n",
    "    for entry_order_delta_mandant in order_delta_mandant_list_merged:\n",
    "        speicher_entry_order_delta_mandant = df_invoicenumber_order_delta_mandant[df_invoicenumber_order_delta_mandant.DocumentID == ''.join(str(entry_order_delta_mandant))]\n",
    "        if (((speicher_entry_order_delta_mandant.Attribute_Name == value1) & (speicher_entry_order_delta_mandant.Delta == bool1))&((speicher_entry_order_delta_mandant.Attribute_Name == value2) & (speicher_entry_order_delta_mandant.Delta == bool2))).any(): \n",
    "            if ((speicher_entry_order_delta_mandant.Attribute_Name == value3) & (speicher_entry_order_delta_mandant.Delta == False)).any():\n",
    "                    counter_vom += 1\n",
    "\n",
    "    #if((counter_vom == 0) | (denominator_order_delta_mandant_num == 0 & counter_vom == 0)):\n",
    "    try:\n",
    "        return counter_vom/denominator_order_delta_mandant_num\n",
    "        #return ([counter_vom/denominator_order_delta_mandant_num,counter_vom,denominator_order_delta_mandant_num])\n",
    "    except:\n",
    "         return pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_values_two_booleans2(df, items):    \n",
    "    counter_vom=0\n",
    "\n",
    "    df_order_mandant_delta = df.loc[(df['Attribute_Name'] == items[0])&(df['Delta'] == items[1])]\n",
    "    df_order_delta_new = df_order_mandant_delta.loc[(df_order_mandant_delta['Attribute_Name'] == items[2])&(df['Delta'] == items[3])]\n",
    "    df_order_delta_mandant_documentid = df_order_delta_new[['DocumentID']]\n",
    "    order_delta_mandant_list = df_order_delta_mandant_documentid.values.tolist()\n",
    "    order_delta_mandant_list_merged = list(itertools.chain.from_iterable(order_delta_mandant_list))\n",
    "    denominator_order_delta_mandant_num = len(order_delta_mandant_list_merged)\n",
    "\n",
    "\n",
    "    df_invoicenumber_order_delta_mandant = df.loc[(df['Attribute_Name'] == items[0])|(df['Attribute_Name'] == items[2])|(df['Attribute_Name'] == items[4])]\n",
    "    for entry_order_delta_mandant in order_delta_mandant_list_merged:\n",
    "        speicher_entry_order_delta_mandant = df_invoicenumber_order_delta_mandant[df_invoicenumber_order_delta_mandant.DocumentID == ''.join(str(entry_order_delta_mandant))]\n",
    "        if (((speicher_entry_order_delta_mandant.Attribute_Name == items[0]) & (speicher_entry_order_delta_mandant.Delta == items[1]))&((speicher_entry_order_delta_mandant.Attribute_Name == items[2]) & (speicher_entry_order_delta_mandant.Delta == items[3]))).any(): \n",
    "            if ((speicher_entry_order_delta_mandant.Attribute_Name == items[4]) & (speicher_entry_order_delta_mandant.Delta == False)).any():\n",
    "                    counter_vom += 1\n",
    "\n",
    "    #if((counter_vom == 0) | (denominator_order_delta_mandant_num == 0 & counter_vom == 0)):\n",
    "    try:\n",
    "        #return counter_vom/denominator_order_delta_mandant_num\n",
    "        return counter_vom/denominator_order_delta_mandant_num, denominator_order_delta_mandant_num\n",
    "    except:\n",
    "         return pd.NA , pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_value(df, value):\n",
    "    df_ordernum = df.loc[(df['Attribute_Name'] == value)] \n",
    "    dist = df_ordernum['Delta'].value_counts(normalize=True)\n",
    "    try:\n",
    "        score_false = dist.loc[False]\n",
    "        if(score_false >= 0):\n",
    "            score = score_false\n",
    "        elif(score_false == 0):\n",
    "            score = dist.loc[True]\n",
    "    except:\n",
    "        return pd.NA , pd.NA\n",
    "\n",
    "        \n",
    "    return score, len(df_ordernum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [['OrderNum',False,'DEBITOR_NUM'],\n",
    "         ['OrderNum',False,'VENDOR_NUM'],\n",
    "         ['OrderNum',True,'DEBITOR_NUM'],\n",
    "         ['VENDOR_NUM',False,'InvoiceNumber'],\n",
    "         ['VENDOR_NUM',True,'InvoiceNumber'],\n",
    "         ['VENDOR_NUM',False,'InvoiceDate'],\n",
    "         ['VENDOR_NUM',True,'InvoiceDate'],\n",
    "         ['VENDOR_NUM',False,'GrossAmount'],\n",
    "         ['VENDOR_NUM',True,'GrossAmount'],\n",
    "         ['VENDOR_NUM',False,'NetAmount1'],\n",
    "         ['VENDOR_NUM',True,'NetAmount1'],\n",
    "         ['VENDOR_NUM',False,'VatAmount1'],\n",
    "         ['VENDOR_NUM',True,'VatAmount1'],\n",
    "         ]\n",
    "\n",
    "list2 = [['OrderNum',True,'DEBITOR_NUM',False,'VENDOR_NUM'],\n",
    "            ['OrderNum',True,'DEBITOR_NUM',True,'VENDOR_NUM'],\n",
    "            ]\n",
    "\n",
    "def get_data_values_complete_results(tenant, splitkey,df_new):\n",
    "\n",
    "    dc_sorted_df = split_datframe_into_whatever(splitkey, df_new)\n",
    "    \n",
    "    df_results = pd.DataFrame(columns=['q(OrderNum)','q(VENDOR_NUM)',\n",
    "            'q(DEBITOR_NUM|OrderNum)',\n",
    "            'q(VENDOR_NUM|OrderNum)',\n",
    "            'q(DEBITOR_NUM|!OrderNum)',\n",
    "            'q(InvoiceNumber|VENDOR_NUM)',\n",
    "            'q(InvoiceNumber|!VENDOR_NUM)',\n",
    "            'q(InvoiceDate|VENDOR_NUM)',\n",
    "            'q(InvoiceDate|!VENDOR_NUM)',\n",
    "            'q(GrossAmount|VENDOR_NUM)',\n",
    "            'q(GrossAmount|!VENDOR_NUM)',\n",
    "            'q(NetAmount1|VENDOR_NUM)',\n",
    "            'q(NetAmount1|!VENDOR_NUM)',\n",
    "            'q(VatAmount1|VENDOR_NUM)',\n",
    "            'q(VatAmount1|!VENDOR_NUM)',\n",
    "            'q(VENDOR_NUM | !OrderNum & DEBITOR_NUM)',\n",
    "            'q(VENDOR_NUM| !OrderNum & ! DEBITOR_NUM)',\n",
    "            ])\n",
    "    \n",
    "    df_results_frequency= pd.DataFrame(columns=['q(OrderNum)','q(VENDOR_NUM)',\n",
    "            'q(DEBITOR_NUM|OrderNum)',\n",
    "            'q(VENDOR_NUM|OrderNum)',\n",
    "            'q(DEBITOR_NUM|!OrderNum)',\n",
    "            'q(InvoiceNumber|VENDOR_NUM)',\n",
    "            'q(InvoiceNumber|!VENDOR_NUM)',\n",
    "            'q(InvoiceDate|VENDOR_NUM)',\n",
    "            'q(InvoiceDate|!VENDOR_NUM)',\n",
    "            'q(GrossAmount|VENDOR_NUM)',\n",
    "            'q(GrossAmount|!VENDOR_NUM)',\n",
    "            'q(NetAmount1|VENDOR_NUM)',\n",
    "            'q(NetAmount1|!VENDOR_NUM)',\n",
    "            'q(VatAmount1|VENDOR_NUM)',\n",
    "            'q(VatAmount1|!VENDOR_NUM)',\n",
    "            'q(VENDOR_NUM | !OrderNum & DEBITOR_NUM)',\n",
    "            'q(VENDOR_NUM| !OrderNum & ! DEBITOR_NUM)',\n",
    "            ])\n",
    "\n",
    "    for key, df in dc_sorted_df.items():\n",
    "    \n",
    "        result_list = []\n",
    "        result_list_frequency = []\n",
    "        #get_sollwerte(key, df)\n",
    "        ordernum, ordernum_frequency = get_single_value(df, 'OrderNum')\n",
    "        result_list.append(ordernum)\n",
    "        result_list_frequency.append(ordernum_frequency)\n",
    "\n",
    "        #print(''+key+': '+'OrderNum = '+str(ordernum)+'')\n",
    "        #Für Simon ai1\n",
    "        #x, y= get_data_values(df, 'VENDOR_NUM', False, 'InvoiceNumber', True)\n",
    "        #print(key)\n",
    "        #print(x,y)\n",
    "\n",
    "        vendornum, vendornum_frequency = get_single_value(df, 'VENDOR_NUM')\n",
    "        #print(''+key+': '+'VENDOR_NUM = '+str(vendornum)+'')\n",
    "        result_list.append(vendornum)\n",
    "        result_list_frequency.append(vendornum_frequency)\n",
    "\n",
    "        for items1 in list1:\n",
    "            value, value_frequency = get_data_values2(df, items1)\n",
    "            result_list.append(value)\n",
    "            result_list_frequency.append(value_frequency)\n",
    "\n",
    "        for items2 in list2:\n",
    "            twovalues, twovalues_frequency = get_data_values_two_booleans2(df, items2)\n",
    "            result_list.append(twovalues)\n",
    "            result_list_frequency.append(twovalues_frequency)\n",
    "            \n",
    "\n",
    "        df_results.loc[key] = result_list\n",
    "        df_results_frequency.loc[key] = result_list_frequency\n",
    "\n",
    "        \n",
    "    return df_results, df_results_frequency ,dc_sorted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sollwerte bestimmen\n",
    "\n",
    "def sollwerte_anhand_wahrscheinlichkeit(df):\n",
    "    sollwerte = pd.Series()\n",
    "    for column in df.columns:\n",
    "        #df.values.tolist()\n",
    "        probability = []\n",
    "        count = []\n",
    "        for entry in df[column]:\n",
    "            #df.values.tolist()\n",
    "            if (pd.isnull(entry)):\n",
    "                continue      \n",
    "            else:\n",
    "                probability.append(entry[0])\n",
    "                count.append(entry[1])\n",
    "\n",
    "        wholecount = sum(count)\n",
    "        gesamtwahrscheinlichkeit = sum([p * h / wholecount for p, h in zip(probability, count)])  #p = probability * count/wholcount\n",
    "\n",
    "        sollwerte[column] = gesamtwahrscheinlichkeit\n",
    "    return sollwerte\n",
    "\n",
    "sollwerte = sollwerte_anhand_wahrscheinlichkeit(df_results)\n",
    "#sollwerte.to_csv('data/sollwerte.csv', index=True, header= True, encoding='utf-8')\n",
    "print(sollwerte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>q(OrderNum)</th>\n",
       "      <th>q(VENDOR_NUM)</th>\n",
       "      <th>q(DEBITOR_NUM|OrderNum)</th>\n",
       "      <th>q(VENDOR_NUM|OrderNum)</th>\n",
       "      <th>q(DEBITOR_NUM|!OrderNum)</th>\n",
       "      <th>q(InvoiceNumber|VENDOR_NUM)</th>\n",
       "      <th>q(InvoiceNumber|!VENDOR_NUM)</th>\n",
       "      <th>q(InvoiceDate|VENDOR_NUM)</th>\n",
       "      <th>q(InvoiceDate|!VENDOR_NUM)</th>\n",
       "      <th>q(GrossAmount|VENDOR_NUM)</th>\n",
       "      <th>q(GrossAmount|!VENDOR_NUM)</th>\n",
       "      <th>q(NetAmount1|VENDOR_NUM)</th>\n",
       "      <th>q(NetAmount1|!VENDOR_NUM)</th>\n",
       "      <th>q(VatAmount1|VENDOR_NUM)</th>\n",
       "      <th>q(VatAmount1|!VENDOR_NUM)</th>\n",
       "      <th>q(VENDOR_NUM | !OrderNum &amp; DEBITOR_NUM)</th>\n",
       "      <th>q(VENDOR_NUM| !OrderNum &amp; ! DEBITOR_NUM)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gewichtung</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Comment</th>\n",
       "      <td>Regulären Ausdruck</td>\n",
       "      <td>variable</td>\n",
       "      <td>wenn nicht stammdatenproblem/falsche bestelldaten</td>\n",
       "      <td>wenn nicht stammdatenproblem/falsche bestelldaten</td>\n",
       "      <td>stammdaten (Adressdaten) im beleg/kopfbereich</td>\n",
       "      <td>Autotrainerproblem</td>\n",
       "      <td>generische Rechnungsnummersuche über Schlüssel...</td>\n",
       "      <td>Autotrainerproblem</td>\n",
       "      <td>generische Rechnungsnummersuche über Schlüssel...</td>\n",
       "      <td>Autotrainerproblem</td>\n",
       "      <td>Betragserkennung</td>\n",
       "      <td>Autotrainerproblem</td>\n",
       "      <td>Betragserkennung abhängig von Steuersatz</td>\n",
       "      <td>Autotrainerproblem</td>\n",
       "      <td>Betragserkennung abhängig von Steuersatz</td>\n",
       "      <td>Lieferantenstammdaten und Bestellstammdaten/se...</td>\n",
       "      <td>Lieferantenstammdaten/semantische Klassifizier...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Unnamed: 0         q(OrderNum) q(VENDOR_NUM)  \\\n",
       "0                          1.0           0.9   \n",
       "Gewichtung                 1.0           2.0   \n",
       "Comment     Regulären Ausdruck      variable   \n",
       "\n",
       "Unnamed: 0                            q(DEBITOR_NUM|OrderNum)  \\\n",
       "0                                                        0.99   \n",
       "Gewichtung                                                2.0   \n",
       "Comment     wenn nicht stammdatenproblem/falsche bestelldaten   \n",
       "\n",
       "Unnamed: 0                             q(VENDOR_NUM|OrderNum)  \\\n",
       "0                                                        0.98   \n",
       "Gewichtung                                                1.0   \n",
       "Comment     wenn nicht stammdatenproblem/falsche bestelldaten   \n",
       "\n",
       "Unnamed: 0                       q(DEBITOR_NUM|!OrderNum)  \\\n",
       "0                                                    0.95   \n",
       "Gewichtung                                            2.0   \n",
       "Comment     stammdaten (Adressdaten) im beleg/kopfbereich   \n",
       "\n",
       "Unnamed: 0 q(InvoiceNumber|VENDOR_NUM)  \\\n",
       "0                                 0.95   \n",
       "Gewichtung                         1.0   \n",
       "Comment             Autotrainerproblem   \n",
       "\n",
       "Unnamed: 0                       q(InvoiceNumber|!VENDOR_NUM)  \\\n",
       "0                                                       0.906   \n",
       "Gewichtung                                                1.0   \n",
       "Comment     generische Rechnungsnummersuche über Schlüssel...   \n",
       "\n",
       "Unnamed: 0 q(InvoiceDate|VENDOR_NUM)  \\\n",
       "0                                0.9   \n",
       "Gewichtung                       1.0   \n",
       "Comment           Autotrainerproblem   \n",
       "\n",
       "Unnamed: 0                         q(InvoiceDate|!VENDOR_NUM)  \\\n",
       "0                                                        0.85   \n",
       "Gewichtung                                                0.5   \n",
       "Comment     generische Rechnungsnummersuche über Schlüssel...   \n",
       "\n",
       "Unnamed: 0 q(GrossAmount|VENDOR_NUM) q(GrossAmount|!VENDOR_NUM)  \\\n",
       "0                               0.95                        0.9   \n",
       "Gewichtung                       2.0                        1.0   \n",
       "Comment           Autotrainerproblem           Betragserkennung   \n",
       "\n",
       "Unnamed: 0 q(NetAmount1|VENDOR_NUM)                 q(NetAmount1|!VENDOR_NUM)  \\\n",
       "0                               0.9                                       0.8   \n",
       "Gewichtung                      0.7                                       0.7   \n",
       "Comment          Autotrainerproblem  Betragserkennung abhängig von Steuersatz   \n",
       "\n",
       "Unnamed: 0 q(VatAmount1|VENDOR_NUM)                 q(VatAmount1|!VENDOR_NUM)  \\\n",
       "0                               0.9                                       0.8   \n",
       "Gewichtung                      0.7                                       0.7   \n",
       "Comment          Autotrainerproblem  Betragserkennung abhängig von Steuersatz   \n",
       "\n",
       "Unnamed: 0            q(VENDOR_NUM | !OrderNum & DEBITOR_NUM)  \\\n",
       "0                                                         0.8   \n",
       "Gewichtung                                                1.0   \n",
       "Comment     Lieferantenstammdaten und Bestellstammdaten/se...   \n",
       "\n",
       "Unnamed: 0           q(VENDOR_NUM| !OrderNum & ! DEBITOR_NUM)  \n",
       "0                                                         0.5  \n",
       "Gewichtung                                                1.0  \n",
       "Comment     Lieferantenstammdaten/semantische Klassifizier...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sollwerte = pd.read_csv('data/sollwerte.csv', encoding='utf-8')\n",
    "sollwerte_transposed = sollwerte.set_index('Unnamed: 0').T\n",
    "display(sollwerte_transposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_element(row):\n",
    "    return row[0]\n",
    "\n",
    "def get_outliers(df_results):\n",
    "    outliers_dataframe = pd.Series([],dtype=pd.StringDtype())\n",
    "\n",
    "    for col in sollwerte_transposed:\n",
    "        #print(sollwerte_transposed[col])\n",
    "        std = df_results[col].std()\n",
    "        sollwert = sollwerte_transposed.iloc[0][col]\n",
    "        gewichtung = sollwerte_transposed.iloc[1][col]\n",
    "        absoluter_abstand = np.abs(sollwert - df_results[col])\n",
    "        outliers = df_results[(df_results[col] - sollwert).abs() > 0.3] #2 * std ((1-sollwert)/2)\n",
    "        #print(outliers.index.tolist())\n",
    "        if not outliers.empty:\n",
    "            outliers_dataframe[col] = outliers.index.tolist()\n",
    "            #print(f\"Ausreißer in Spalte {col}:\")\n",
    "            #print(outliers.index)\n",
    "            #return(col, outliers.index)\n",
    "\n",
    "    return(outliers_dataframe)\n",
    "\n",
    "#median = df_results.median()\n",
    "\n",
    "def sort_outliers(outliers_results, df_results_frequency):\n",
    "    sorted_outliers_dataframe = pd.Series([],dtype=pd.StringDtype())\n",
    "    for entry in outliers_results.items():\n",
    "        filtered_df2 = df_results_frequency[df_results_frequency.index.isin(entry[1])]\n",
    "        sorted_filtered_df2 = filtered_df2.sort_values(by=entry[0], ascending=False)\n",
    "        sorted_filtered_df2 = sorted_filtered_df2[entry[0]]\n",
    "        sorted_outliers_dataframe[entry[0]]  = sorted_filtered_df2.index.tolist()\n",
    "    #print(df_results_debitor_frequency[entry[0]].sort_values(entry[1]))\n",
    "    return sorted_outliers_dataframe\n",
    "    #entry[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_numbers_by_position(df, sollwerte_transposed ,col_name, threshold):\n",
    "    count_numbers = {}\n",
    "    positions = {}\n",
    "    for i, row in df.iterrows():\n",
    "        gewichtung = sollwerte_transposed.iloc[1][i]\n",
    "        for number in row[col_name]:\n",
    "            if number in count_numbers:\n",
    "                count_numbers[number] += 1 * gewichtung\n",
    "            else:\n",
    "                count_numbers[number] = 1\n",
    "            if number in positions:\n",
    "                positions[number].append(row[col_name].index(number))\n",
    "            else:\n",
    "                positions[number] = [row[col_name].index(number)]\n",
    "    \n",
    "    sorted_counts = sorted(count_numbers.items(), key=lambda x: sum(positions[x[0]]) / len(positions[x[0]]))\n",
    "    sorted_numbers = [x[0] for x in sorted_counts]\n",
    "    \n",
    "    threshold_numbers = [x[0] for x in sorted_counts if x[1] > threshold]\n",
    "    \n",
    "    return sorted_numbers, threshold_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate = \"'2022-12-20 09:44:23.030'\"\n",
    "enddate = \"'2023-04-19 12:28:20.000'\"\n",
    "\n",
    "df_table_cclogattributes = get_table_data_CCLOG('CCLogAttributes', connect_to_db_better(connection_string= 'classconprocessingger.database.windows.net', database = 'T_'+tenant+'')\n",
    "                                           ,\"\"+startdate+\"\",\"\"+enddate+\"\")\n",
    "df_table_cclogattributes = df_table_cclogattributes.drop(['Zone','LogTime','Attribute_DataType','LogTimeTicks'], axis=1)\n",
    "df_table_cclogattributes = df_table_cclogattributes.replace('\\n',' ', regex=True)\n",
    "df_table_cclogattributes = df_table_cclogattributes.replace('\\r',' ', regex=True)\n",
    "df_table_cclogattributes.to_csv('./data/cclogattributes_T'+tenant+'_reduced.csv', index=False, header= True, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Mandant Lieferant Fehlercode DocumentID MissingCode\n",
      "0  137 Belagswerk Oberland AG       NaN        NaN        NaN         NaN\n"
     ]
    }
   ],
   "source": [
    "df_scorecard_dataframe = pd.DataFrame(columns=['Mandant','Lieferant','Fehlercode','DocumentID','MissingCode'])\n",
    "df_scorecard_dataframe.loc[0, 'Mandant'] = '137 Belagswerk Oberland AG'\n",
    "print(df_scorecard_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['209786', '200590', '245717', '213016', '200978', '212530', '202085', '248818', '401353']\n",
      "['500926', '506604', '506589']\n",
      "['234864', '201792', '219864', '247532', '300164']\n",
      "['506178']\n",
      "['201613', '247045']\n",
      "['605458']\n"
     ]
    }
   ],
   "source": [
    "score_card = pd.Series([],dtype=pd.StringDtype())\n",
    "df_scorecard_dataframe = pd.DataFrame(columns=['Mandant','Lieferant','Fehlercode','DocumentID','MissingCode'])\n",
    "\n",
    "sollwerte = pd.read_csv('data/sollwerte.csv', encoding='utf-8')\n",
    "sollwerte_transposed = sollwerte.set_index('Unnamed: 0').T\n",
    "dc_sorted_df_vendor_complete = dict()\n",
    "\n",
    "\n",
    "df_results_debitor,df_results_frequency ,dc_sorted_df_debitor = get_data_values_complete_results(tenant, 'DEBITOR_NUM', df_table_cclogattributes)\n",
    "outliers_results = get_outliers(df_results_debitor)\n",
    "outliers_results_sorted_debitor = sort_outliers(outliers_results, df_results_frequency)\n",
    "outliers_results_debitor_frame = outliers_results_sorted_debitor.to_frame()\n",
    "sorted_counts, high_frequency_numbers_debitor = sort_numbers_by_position(outliers_results_debitor_frame, sollwerte_transposed,0, threshold=2)\n",
    "\n",
    "bad_vendors = []\n",
    "count = 0\n",
    "for entry in high_frequency_numbers_debitor:\n",
    "    dc_sorted_df_vendor_complete[entry] = {}\n",
    "    df_results_vendor, df_results_frequency_vendor, dc_sorted_df_vendor = get_data_values_complete_results(tenant, 'VENDOR_NUM', dc_sorted_df_debitor[entry])\n",
    "    dc_sorted_df_vendor_complete[entry].update(dc_sorted_df_vendor)\n",
    "    outliers_results_vendor = get_outliers(df_results_vendor)\n",
    "    outliers_results_sorted_vendor = sort_outliers(outliers_results_vendor, df_results_frequency_vendor)\n",
    "    outliers_results_vendor_frame = outliers_results_sorted_vendor.to_frame()\n",
    "    sorted_counts_vendor, high_frequency_numbers_vendor = sort_numbers_by_position(outliers_results_vendor_frame, sollwerte_transposed, 0, threshold=2)\n",
    "    bad_vendors.append(high_frequency_numbers_vendor)\n",
    "    print(high_frequency_numbers_vendor)\n",
    "    \n",
    "    for entry_high_frequency_numbers_vendor in high_frequency_numbers_vendor:\n",
    "        df_scorecard_dataframe.loc[count, 'Mandant'] = entry\n",
    "        df_scorecard_dataframe.loc[count, 'Lieferant'] = entry_high_frequency_numbers_vendor \n",
    "        count += 1\n",
    "        \n",
    "    score_card[entry]  = high_frequency_numbers_vendor\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table_ccvendors = get_table_data_ALL('CC_VENDORS', connect_to_db_better(connection_string= 'classconprocessingger.database.windows.net', database = 'T_'+tenant+''))\n",
    "df_table_ccvendors_bank = get_table_data_ALL('CC_VENDOR_BANK', connect_to_db_better(connection_string= 'classconprocessingger.database.windows.net', database = 'T_'+tenant+''))\n",
    "\n",
    "df_table_ccvendors = df_table_ccvendors.replace('\\n',' ', regex=True)\n",
    "df_table_ccvendors = df_table_ccvendors.replace('\\r',' ', regex=True)\n",
    "df_table_ccvendors_bank = df_table_ccvendors_bank.replace('\\n',' ', regex=True)\n",
    "df_table_ccvendors_bank = df_table_ccvendors_bank.replace('\\r',' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_eap_error(vendor_num, debitor):  \n",
    "        df_sort = dc_sorted_df_vendor_complete[debitor][vendor_num]\n",
    "        df_sort_vendor = df_sort[((df_sort['Attribute_Name'] == 'VENDOR_NUM') & (df_sort['Delta'] == False))]\n",
    "        documentid_list = df_sort_vendor[['DocumentID']].values.tolist()\n",
    "        merged = list(itertools.chain.from_iterable(documentid_list))\n",
    "        df_sorted = df_sort[df_sort['DocumentID'].isin(merged)]\n",
    "\n",
    "        df_sort_new = df_sorted[((df_sorted['Attribute_Name'] == 'VatAmount1') & (df_sorted['Delta'] == True)) | (df_sorted['Attribute_Name'] == 'NetAmount1') & (df_sorted['Delta'] == True) | (df_sorted['Attribute_Name'] == 'VatRate1') & (df_sorted['Delta'] == True)]\n",
    "\n",
    "        unique_documentid = df_sort_new['DocumentID'].unique()\n",
    "        without_vat = [x for x in merged if x not in unique_documentid]\n",
    "        \n",
    "        for document_id in without_vat:\n",
    "                df_sorted = df_sort[(df_sort[\"DocumentID\"] == document_id) & (df_sort[\"Delta\"] == True)]     \n",
    "                #display(df_sorted)     \n",
    "\n",
    "        wrong_documentids = []\n",
    "        error_codes = []\n",
    "\n",
    "        for entry in unique_documentid:\n",
    "                sorted_by_documentid = df_sort_new.loc[(df_sort_new[\"DocumentID\"] == entry)]\n",
    "                len_sorted_by_documentid = len(sorted_by_documentid)\n",
    "                sorted_by_documentid = sorted_by_documentid.loc[(sorted_by_documentid[\"Attribute_Name\"] == 'VatAmount1')]\n",
    "                if not sorted_by_documentid.empty:\n",
    "                        if 0.0 < abs(float(sorted_by_documentid[\"Attribute_After\"]) - float(sorted_by_documentid[\"Attribute_Before\"])) <= 0.05:\n",
    "                                #print('Bei: '+entry+' Rundungsfehler')\n",
    "                                x=1\n",
    "\n",
    "                if not sorted_by_documentid.empty:\n",
    "                        wrong_documentids.append(entry)\n",
    "                        error_codes.append(len_sorted_by_documentid)\n",
    "\n",
    "        return wrong_documentids, error_codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 Belagswerk Oberland AG      [209786, 111, [[0bbb42b7-fc87-4979-ae70-ef0044...\n",
      "640 Renesco AT                  [500926, 111, 506604, 111, [[79501357-d8c4-422...\n",
      "181 Renesco AG                  [234864, 10, [[d6f86eda-ce72-420a-9169-a5ea8c7...\n",
      "402                                                                 [506178, 111]\n",
      "427 SG                          [201613, 111, 247045, 111, [[e9c4f751-ffde-4ba...\n",
      "904 ARGE Umfahrung Mellingen                                        [605458, 111]\n",
      "dtype: object\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "score_card_missing_vendor_vat_registration_id = pd.Series([],dtype=pd.StringDtype())\n",
    "\n",
    "#score_card_missing_vendor_vat_registration_id = pd.DataFrame()\n",
    "fehlercount = 0\n",
    "bad_documents_counter = pd.Series([],dtype=pd.StringDtype()) \n",
    "for entry in score_card:\n",
    "    wrong_number = []\n",
    "    wrong_documentids = []\n",
    "\n",
    "    for item in entry:\n",
    "        #wrong_documentids.append(check_for_eap_error(item, score_card.index[count]))\n",
    "        wrong_documentids = (check_for_eap_error(item, score_card.index[count]))\n",
    "        entry_wrong_number_vat_registration_id = df_table_ccvendors[(df_table_ccvendors['COMPANY_NUM'] == score_card.index[count]) & (df_table_ccvendors[\"VENDOR_NUM\"] == item)& ~(df_table_ccvendors[\"VENDOR_VAT_REGISTRATION_ID\"] == '')]\n",
    "        entry_wrong_number_registration_id = df_table_ccvendors[(df_table_ccvendors['COMPANY_NUM'] == score_card.index[count]) & (df_table_ccvendors[\"VENDOR_NUM\"] == item)& ~(df_table_ccvendors[\"VENDOR_REGISTRATION_ID\"] == '')] \n",
    "        entry_wrong_number_iban = df_table_ccvendors_bank[(df_table_ccvendors_bank['COMPANY_NUM'] == score_card.index[count]) & (df_table_ccvendors_bank[\"VENDOR_NUM\"] == item)& ~(df_table_ccvendors_bank[\"IBAN\"] == '')]\n",
    "        if entry_wrong_number_vat_registration_id.empty: \n",
    "            if item not in wrong_number:\n",
    "                wrong_number.append(item) \n",
    "            fehlercount += 100\n",
    "        if entry_wrong_number_registration_id.empty: \n",
    "            if item not in wrong_number:\n",
    "                wrong_number.append(item)\n",
    "            fehlercount += 10\n",
    "        if entry_wrong_number_iban.empty: \n",
    "            if item not in wrong_number:\n",
    "                wrong_number.append(item)\n",
    "            fehlercount += 1\n",
    "\n",
    "        wrong_number.append(fehlercount)\n",
    "        if wrong_documentids:\n",
    "            bad_documents_counter[score_card.index[count]] = len(wrong_documentids)\n",
    "            wrong_number.append(wrong_documentids)\n",
    "\n",
    "        fehlercount = 0\n",
    "    #print(wrong_documentids)\n",
    "\n",
    "    score_card_missing_vendor_vat_registration_id[score_card.index[count]] = wrong_number\n",
    "    count += 1\n",
    "\n",
    "print(score_card_missing_vendor_vat_registration_id)\n",
    "print(score_card_missing_vendor_vat_registration_id.to_json('data/scorecard_verbesserung.json', index=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['605458', 111, 111, 222, 333, [['0bbb42b7-fc87-4979-ae70-ef0044771515', 3]], 111, [['b376f430-3a29-4dfa-b465-7c1a54f68355', 1], ['a9dd486d-c2cf-4a76-82e2-dd1beda9c729', 2], ['e348bc24-20b5-4ace-b429-2d3183eebd75', 2]], 111, 111, 111, [['79da9af7-ace6-4bd9-a3be-cd6b132d073e', 1], ['415f8cfa-5fb5-464c-94ea-dc733d9074df', 1]], 111, [['5a90b8d7-7634-4113-b572-58f29f46d8e5', 3], ['67097e2d-6eba-4037-bc15-9d5d5e3f55dd', 2], ['fd31ac12-735f-4e9a-9a0a-8c2f53b5f4b5', 2]], 111, [['78ffdfae-cdb7-4b0f-b00f-7a7ac8662e7f', 1]], 111, 111, 111, 111, [['79501357-d8c4-422a-9cd9-13d23d2a6044', 3], ['01d94a06-7215-42d7-8715-23e46bc4ebcd', 3]], 10, 10, [['d6f86eda-ce72-420a-9169-a5ea8c7a5811', 1], ['e65d17f2-4db8-4dfd-9070-dc422590ac06', 1], ['84735480-8d1f-49d7-8b5f-06096659d070', 1], ['b57ee07f-7dec-41f4-9edf-e1246dc1274b', 1], ['a7ad4b6c-a6a6-43d0-ab1a-f499e22dc7ac', 1], ['560fd7cb-4569-4700-8ad8-bb6ef01e2378', 1], ['896c2248-1b77-4eb7-9abb-9db6076d4339', 1], ['85c937d7-fcde-4171-8b06-6e93adcdbf88', 1]], 110, 10, 10, [['e2e9d750-0cf4-49a7-8efa-8b42f3053a34', 3]], 111, [['a712b491-eb19-48cd-88bb-bdb0077a83e6', 3]], 111, 111, 111, [['e9c4f751-ffde-4bab-a221-c132a3c438d5', 3]], 111, [['0bbb42b7-fc87-4979-ae70-ef0044771515', 3]], 111, [['b376f430-3a29-4dfa-b465-7c1a54f68355', 1], ['a9dd486d-c2cf-4a76-82e2-dd1beda9c729', 2], ['e348bc24-20b5-4ace-b429-2d3183eebd75', 2]], 111, 111, 111, [['79da9af7-ace6-4bd9-a3be-cd6b132d073e', 1], ['415f8cfa-5fb5-464c-94ea-dc733d9074df', 1]], 111, [['5a90b8d7-7634-4113-b572-58f29f46d8e5', 3], ['67097e2d-6eba-4037-bc15-9d5d5e3f55dd', 2], ['fd31ac12-735f-4e9a-9a0a-8c2f53b5f4b5', 2]], 111, [['78ffdfae-cdb7-4b0f-b00f-7a7ac8662e7f', 1]], 111, 111, 111, 111, [['79501357-d8c4-422a-9cd9-13d23d2a6044', 3], ['01d94a06-7215-42d7-8715-23e46bc4ebcd', 3]], 111, 10, [['d6f86eda-ce72-420a-9169-a5ea8c7a5811', 1], ['e65d17f2-4db8-4dfd-9070-dc422590ac06', 1], ['84735480-8d1f-49d7-8b5f-06096659d070', 1], ['b57ee07f-7dec-41f4-9edf-e1246dc1274b', 1], ['a7ad4b6c-a6a6-43d0-ab1a-f499e22dc7ac', 1], ['560fd7cb-4569-4700-8ad8-bb6ef01e2378', 1], ['896c2248-1b77-4eb7-9abb-9db6076d4339', 1], ['85c937d7-fcde-4171-8b06-6e93adcdbf88', 1]], 10, 110, 10, [['e2e9d750-0cf4-49a7-8efa-8b42f3053a34', 3]], 10, [['a712b491-eb19-48cd-88bb-bdb0077a83e6', 3]], 111, 111, 111, [['e9c4f751-ffde-4bab-a221-c132a3c438d5', 3]], 111, 111, [['0bbb42b7-fc87-4979-ae70-ef0044771515', 3]], 111, [['b376f430-3a29-4dfa-b465-7c1a54f68355', 1], ['a9dd486d-c2cf-4a76-82e2-dd1beda9c729', 2], ['e348bc24-20b5-4ace-b429-2d3183eebd75', 2]], 111, 111, 111, [['79da9af7-ace6-4bd9-a3be-cd6b132d073e', 1], ['415f8cfa-5fb5-464c-94ea-dc733d9074df', 1]], 111, [['5a90b8d7-7634-4113-b572-58f29f46d8e5', 3], ['67097e2d-6eba-4037-bc15-9d5d5e3f55dd', 2], ['fd31ac12-735f-4e9a-9a0a-8c2f53b5f4b5', 2]], 111, [['78ffdfae-cdb7-4b0f-b00f-7a7ac8662e7f', 1]], 111, 111, 111, 111, [['79501357-d8c4-422a-9cd9-13d23d2a6044', 3], ['01d94a06-7215-42d7-8715-23e46bc4ebcd', 3]], 111, 10, [['d6f86eda-ce72-420a-9169-a5ea8c7a5811', 1], ['e65d17f2-4db8-4dfd-9070-dc422590ac06', 1], ['84735480-8d1f-49d7-8b5f-06096659d070', 1], ['b57ee07f-7dec-41f4-9edf-e1246dc1274b', 1], ['a7ad4b6c-a6a6-43d0-ab1a-f499e22dc7ac', 1], ['560fd7cb-4569-4700-8ad8-bb6ef01e2378', 1], ['896c2248-1b77-4eb7-9abb-9db6076d4339', 1], ['85c937d7-fcde-4171-8b06-6e93adcdbf88', 1]], 10, 110, 10, [['e2e9d750-0cf4-49a7-8efa-8b42f3053a34', 3]], 10, [['a712b491-eb19-48cd-88bb-bdb0077a83e6', 3]], 111, 111, 111, [['e9c4f751-ffde-4bab-a221-c132a3c438d5', 3]], 111, 111, [['0bbb42b7-fc87-4979-ae70-ef0044771515', 3]], 111, [['b376f430-3a29-4dfa-b465-7c1a54f68355', 1], ['a9dd486d-c2cf-4a76-82e2-dd1beda9c729', 2], ['e348bc24-20b5-4ace-b429-2d3183eebd75', 2]], 111, 111, 111, [['79da9af7-ace6-4bd9-a3be-cd6b132d073e', 1], ['415f8cfa-5fb5-464c-94ea-dc733d9074df', 1]], 111, [['5a90b8d7-7634-4113-b572-58f29f46d8e5', 3], ['67097e2d-6eba-4037-bc15-9d5d5e3f55dd', 2], ['fd31ac12-735f-4e9a-9a0a-8c2f53b5f4b5', 2]], 111, [['78ffdfae-cdb7-4b0f-b00f-7a7ac8662e7f', 1]], 111, 111, 111, 111, [['79501357-d8c4-422a-9cd9-13d23d2a6044', 3], ['01d94a06-7215-42d7-8715-23e46bc4ebcd', 3]], 111, 10, [['d6f86eda-ce72-420a-9169-a5ea8c7a5811', 1], ['e65d17f2-4db8-4dfd-9070-dc422590ac06', 1], ['84735480-8d1f-49d7-8b5f-06096659d070', 1], ['b57ee07f-7dec-41f4-9edf-e1246dc1274b', 1], ['a7ad4b6c-a6a6-43d0-ab1a-f499e22dc7ac', 1], ['560fd7cb-4569-4700-8ad8-bb6ef01e2378', 1], ['896c2248-1b77-4eb7-9abb-9db6076d4339', 1], ['85c937d7-fcde-4171-8b06-6e93adcdbf88', 1]], 10, 110, 10, [['e2e9d750-0cf4-49a7-8efa-8b42f3053a34', 3]], 10, [['a712b491-eb19-48cd-88bb-bdb0077a83e6', 3]], 111, 111, 111, [['e9c4f751-ffde-4bab-a221-c132a3c438d5', 3]], 111, 111, [['0bbb42b7-fc87-4979-ae70-ef0044771515', 3]], 111, [['b376f430-3a29-4dfa-b465-7c1a54f68355', 1], ['a9dd486d-c2cf-4a76-82e2-dd1beda9c729', 2], ['e348bc24-20b5-4ace-b429-2d3183eebd75', 2]], 111, 111, 111, [['79da9af7-ace6-4bd9-a3be-cd6b132d073e', 1], ['415f8cfa-5fb5-464c-94ea-dc733d9074df', 1]], 111, [['5a90b8d7-7634-4113-b572-58f29f46d8e5', 3], ['67097e2d-6eba-4037-bc15-9d5d5e3f55dd', 2], ['fd31ac12-735f-4e9a-9a0a-8c2f53b5f4b5', 2]], 111, [['78ffdfae-cdb7-4b0f-b00f-7a7ac8662e7f', 1]], 111, 111, 111, 111, [['79501357-d8c4-422a-9cd9-13d23d2a6044', 3], ['01d94a06-7215-42d7-8715-23e46bc4ebcd', 3]], 111, 10, [['d6f86eda-ce72-420a-9169-a5ea8c7a5811', 1], ['e65d17f2-4db8-4dfd-9070-dc422590ac06', 1], ['84735480-8d1f-49d7-8b5f-06096659d070', 1], ['b57ee07f-7dec-41f4-9edf-e1246dc1274b', 1], ['a7ad4b6c-a6a6-43d0-ab1a-f499e22dc7ac', 1], ['560fd7cb-4569-4700-8ad8-bb6ef01e2378', 1], ['896c2248-1b77-4eb7-9abb-9db6076d4339', 1], ['85c937d7-fcde-4171-8b06-6e93adcdbf88', 1]], 10, 110, 10, [['e2e9d750-0cf4-49a7-8efa-8b42f3053a34', 3]], 10, [['a712b491-eb19-48cd-88bb-bdb0077a83e6', 3]], 111, 111, 111, [['e9c4f751-ffde-4bab-a221-c132a3c438d5', 3]], 111, 111, [['0bbb42b7-fc87-4979-ae70-ef0044771515', 3]], 111, [['b376f430-3a29-4dfa-b465-7c1a54f68355', 1], ['a9dd486d-c2cf-4a76-82e2-dd1beda9c729', 2], ['e348bc24-20b5-4ace-b429-2d3183eebd75', 2]], 111, 111, 111, [['79da9af7-ace6-4bd9-a3be-cd6b132d073e', 1], ['415f8cfa-5fb5-464c-94ea-dc733d9074df', 1]], 111, [['5a90b8d7-7634-4113-b572-58f29f46d8e5', 3], ['67097e2d-6eba-4037-bc15-9d5d5e3f55dd', 2], ['fd31ac12-735f-4e9a-9a0a-8c2f53b5f4b5', 2]], 111, [['78ffdfae-cdb7-4b0f-b00f-7a7ac8662e7f', 1]], 111, 111, 111, 111, [['79501357-d8c4-422a-9cd9-13d23d2a6044', 3], ['01d94a06-7215-42d7-8715-23e46bc4ebcd', 3]], 111, 10, [['d6f86eda-ce72-420a-9169-a5ea8c7a5811', 1], ['e65d17f2-4db8-4dfd-9070-dc422590ac06', 1], ['84735480-8d1f-49d7-8b5f-06096659d070', 1], ['b57ee07f-7dec-41f4-9edf-e1246dc1274b', 1], ['a7ad4b6c-a6a6-43d0-ab1a-f499e22dc7ac', 1], ['560fd7cb-4569-4700-8ad8-bb6ef01e2378', 1], ['896c2248-1b77-4eb7-9abb-9db6076d4339', 1], ['85c937d7-fcde-4171-8b06-6e93adcdbf88', 1]], 10, 110, 10, [['e2e9d750-0cf4-49a7-8efa-8b42f3053a34', 3]], 10, [['a712b491-eb19-48cd-88bb-bdb0077a83e6', 3]], 111, 111, 111, [['e9c4f751-ffde-4bab-a221-c132a3c438d5', 3]], 111, 111, [['0bbb42b7-fc87-4979-ae70-ef0044771515', 3]], 111, [['b376f430-3a29-4dfa-b465-7c1a54f68355', 1], ['a9dd486d-c2cf-4a76-82e2-dd1beda9c729', 2], ['e348bc24-20b5-4ace-b429-2d3183eebd75', 2]], 111, 111, 111, [['79da9af7-ace6-4bd9-a3be-cd6b132d073e', 1], ['415f8cfa-5fb5-464c-94ea-dc733d9074df', 1]], 111, [['5a90b8d7-7634-4113-b572-58f29f46d8e5', 3], ['67097e2d-6eba-4037-bc15-9d5d5e3f55dd', 2], ['fd31ac12-735f-4e9a-9a0a-8c2f53b5f4b5', 2]], 111, [['78ffdfae-cdb7-4b0f-b00f-7a7ac8662e7f', 1]], 111, 111, 111, 111, [['79501357-d8c4-422a-9cd9-13d23d2a6044', 3], ['01d94a06-7215-42d7-8715-23e46bc4ebcd', 3]], 111, 10, [['d6f86eda-ce72-420a-9169-a5ea8c7a5811', 1], ['e65d17f2-4db8-4dfd-9070-dc422590ac06', 1], ['84735480-8d1f-49d7-8b5f-06096659d070', 1], ['b57ee07f-7dec-41f4-9edf-e1246dc1274b', 1], ['a7ad4b6c-a6a6-43d0-ab1a-f499e22dc7ac', 1], ['560fd7cb-4569-4700-8ad8-bb6ef01e2378', 1], ['896c2248-1b77-4eb7-9abb-9db6076d4339', 1], ['85c937d7-fcde-4171-8b06-6e93adcdbf88', 1]], 10, 110, 10, [['e2e9d750-0cf4-49a7-8efa-8b42f3053a34', 3]], 10, [['a712b491-eb19-48cd-88bb-bdb0077a83e6', 3]], 111, 111, 111, [['e9c4f751-ffde-4bab-a221-c132a3c438d5', 3]], 111, 111, [['0bbb42b7-fc87-4979-ae70-ef0044771515', 3]], 111, [['b376f430-3a29-4dfa-b465-7c1a54f68355', 1], ['a9dd486d-c2cf-4a76-82e2-dd1beda9c729', 2], ['e348bc24-20b5-4ace-b429-2d3183eebd75', 2]], 111, 111, 111, [['79da9af7-ace6-4bd9-a3be-cd6b132d073e', 1], ['415f8cfa-5fb5-464c-94ea-dc733d9074df', 1]], 111, [['5a90b8d7-7634-4113-b572-58f29f46d8e5', 3], ['67097e2d-6eba-4037-bc15-9d5d5e3f55dd', 2], ['fd31ac12-735f-4e9a-9a0a-8c2f53b5f4b5', 2]], 111, [['78ffdfae-cdb7-4b0f-b00f-7a7ac8662e7f', 1]], 111, 111, 111, 111, [['79501357-d8c4-422a-9cd9-13d23d2a6044', 3], ['01d94a06-7215-42d7-8715-23e46bc4ebcd', 3]], 111, 10, [['d6f86eda-ce72-420a-9169-a5ea8c7a5811', 1], ['e65d17f2-4db8-4dfd-9070-dc422590ac06', 1], ['84735480-8d1f-49d7-8b5f-06096659d070', 1], ['b57ee07f-7dec-41f4-9edf-e1246dc1274b', 1], ['a7ad4b6c-a6a6-43d0-ab1a-f499e22dc7ac', 1], ['560fd7cb-4569-4700-8ad8-bb6ef01e2378', 1], ['896c2248-1b77-4eb7-9abb-9db6076d4339', 1], ['85c937d7-fcde-4171-8b06-6e93adcdbf88', 1]], 10, 110, 10, [['e2e9d750-0cf4-49a7-8efa-8b42f3053a34', 3]], 10, [['a712b491-eb19-48cd-88bb-bdb0077a83e6', 3]], 111, 111, 111, [['e9c4f751-ffde-4bab-a221-c132a3c438d5', 3]], 111, 111, [['0bbb42b7-fc87-4979-ae70-ef0044771515', 3]], 111, [['b376f430-3a29-4dfa-b465-7c1a54f68355', 1], ['a9dd486d-c2cf-4a76-82e2-dd1beda9c729', 2], ['e348bc24-20b5-4ace-b429-2d3183eebd75', 2]], 111, 111, 111, [['79da9af7-ace6-4bd9-a3be-cd6b132d073e', 1], ['415f8cfa-5fb5-464c-94ea-dc733d9074df', 1]], 111, [['5a90b8d7-7634-4113-b572-58f29f46d8e5', 3], ['67097e2d-6eba-4037-bc15-9d5d5e3f55dd', 2], ['fd31ac12-735f-4e9a-9a0a-8c2f53b5f4b5', 2]], 111, [['78ffdfae-cdb7-4b0f-b00f-7a7ac8662e7f', 1]], 111, 111, 111, 111, [['79501357-d8c4-422a-9cd9-13d23d2a6044', 3], ['01d94a06-7215-42d7-8715-23e46bc4ebcd', 3]], 111, 10, [['d6f86eda-ce72-420a-9169-a5ea8c7a5811', 1], ['e65d17f2-4db8-4dfd-9070-dc422590ac06', 1], ['84735480-8d1f-49d7-8b5f-06096659d070', 1], ['b57ee07f-7dec-41f4-9edf-e1246dc1274b', 1], ['a7ad4b6c-a6a6-43d0-ab1a-f499e22dc7ac', 1], ['560fd7cb-4569-4700-8ad8-bb6ef01e2378', 1], ['896c2248-1b77-4eb7-9abb-9db6076d4339', 1], ['85c937d7-fcde-4171-8b06-6e93adcdbf88', 1]], 10, 110, 10, [['e2e9d750-0cf4-49a7-8efa-8b42f3053a34', 3]], 10, [['a712b491-eb19-48cd-88bb-bdb0077a83e6', 3]], 111, 111, 111, [['e9c4f751-ffde-4bab-a221-c132a3c438d5', 3]], 111, 111, ['0bbb42b7-fc87-4979-ae70-ef0044771515'], 111, ['b376f430-3a29-4dfa-b465-7c1a54f68355', 'a9dd486d-c2cf-4a76-82e2-dd1beda9c729', 'e348bc24-20b5-4ace-b429-2d3183eebd75'], 111, 111, 111, ['79da9af7-ace6-4bd9-a3be-cd6b132d073e', '415f8cfa-5fb5-464c-94ea-dc733d9074df'], 111, ['5a90b8d7-7634-4113-b572-58f29f46d8e5', '67097e2d-6eba-4037-bc15-9d5d5e3f55dd', 'fd31ac12-735f-4e9a-9a0a-8c2f53b5f4b5'], 111, ['78ffdfae-cdb7-4b0f-b00f-7a7ac8662e7f'], 111, 111, 111, 111, ['79501357-d8c4-422a-9cd9-13d23d2a6044', '01d94a06-7215-42d7-8715-23e46bc4ebcd'], 111, 10, ['d6f86eda-ce72-420a-9169-a5ea8c7a5811', 'e65d17f2-4db8-4dfd-9070-dc422590ac06', '84735480-8d1f-49d7-8b5f-06096659d070', 'b57ee07f-7dec-41f4-9edf-e1246dc1274b', 'a7ad4b6c-a6a6-43d0-ab1a-f499e22dc7ac', '560fd7cb-4569-4700-8ad8-bb6ef01e2378', '896c2248-1b77-4eb7-9abb-9db6076d4339', '85c937d7-fcde-4171-8b06-6e93adcdbf88'], 10, 110, 10, ['e2e9d750-0cf4-49a7-8efa-8b42f3053a34'], 10, ['a712b491-eb19-48cd-88bb-bdb0077a83e6'], 111, 111, 111, ['e9c4f751-ffde-4bab-a221-c132a3c438d5'], 111, 111, ['0bbb42b7-fc87-4979-ae70-ef0044771515'], 111, ['b376f430-3a29-4dfa-b465-7c1a54f68355', 'a9dd486d-c2cf-4a76-82e2-dd1beda9c729', 'e348bc24-20b5-4ace-b429-2d3183eebd75'], 111, 111, 111, ['79da9af7-ace6-4bd9-a3be-cd6b132d073e', '415f8cfa-5fb5-464c-94ea-dc733d9074df'], 111, ['5a90b8d7-7634-4113-b572-58f29f46d8e5', '67097e2d-6eba-4037-bc15-9d5d5e3f55dd', 'fd31ac12-735f-4e9a-9a0a-8c2f53b5f4b5'], 111, ['78ffdfae-cdb7-4b0f-b00f-7a7ac8662e7f'], 111, 111, 111, 111, ['79501357-d8c4-422a-9cd9-13d23d2a6044', '01d94a06-7215-42d7-8715-23e46bc4ebcd'], 111, 10, ['d6f86eda-ce72-420a-9169-a5ea8c7a5811', 'e65d17f2-4db8-4dfd-9070-dc422590ac06', '84735480-8d1f-49d7-8b5f-06096659d070', 'b57ee07f-7dec-41f4-9edf-e1246dc1274b', 'a7ad4b6c-a6a6-43d0-ab1a-f499e22dc7ac', '560fd7cb-4569-4700-8ad8-bb6ef01e2378', '896c2248-1b77-4eb7-9abb-9db6076d4339', '85c937d7-fcde-4171-8b06-6e93adcdbf88'], 10, 110, 10, ['e2e9d750-0cf4-49a7-8efa-8b42f3053a34'], 10, ['a712b491-eb19-48cd-88bb-bdb0077a83e6'], 111, 111, 111, ['e9c4f751-ffde-4bab-a221-c132a3c438d5'], 111]\n"
     ]
    }
   ],
   "source": [
    "current_mandant = None\n",
    "current_lieferant = None\n",
    "\n",
    "# Schleife über die Spalten des DataFrames\n",
    "for column in df_scorecard_dataframe.columns:\n",
    "    if column == 'Mandant':\n",
    "        current_mandant = df_scorecard_dataframe[column]\n",
    "    elif column == 'Lieferant':\n",
    "        current_lieferant = df_scorecard_dataframe[column]\n",
    "\n",
    "i = 0\n",
    "\n",
    "while i < len(current_mandant):\n",
    "    wrong_documentids, error_codes = (check_for_eap_error(current_lieferant[i], current_mandant[i]))\n",
    "    entry_wrong_number_vat_registration_id = df_table_ccvendors[(df_table_ccvendors['COMPANY_NUM'] == current_mandant[i]) & (df_table_ccvendors[\"VENDOR_NUM\"] == current_lieferant[i])& ~(df_table_ccvendors[\"VENDOR_VAT_REGISTRATION_ID\"] == '')]\n",
    "    entry_wrong_number_registration_id = df_table_ccvendors[(df_table_ccvendors['COMPANY_NUM'] == current_mandant[i]) & (df_table_ccvendors[\"VENDOR_NUM\"] == current_lieferant[i])& ~(df_table_ccvendors[\"VENDOR_REGISTRATION_ID\"] == '')] \n",
    "    entry_wrong_number_iban = df_table_ccvendors_bank[(df_table_ccvendors_bank['COMPANY_NUM'] == current_mandant[i]) & (df_table_ccvendors_bank[\"VENDOR_NUM\"] == current_lieferant[i])& ~(df_table_ccvendors_bank[\"IBAN\"] == '')]\n",
    "    \n",
    "    if entry_wrong_number_vat_registration_id.empty: \n",
    "        if item not in wrong_number:\n",
    "            wrong_number.append(item) \n",
    "        fehlercount += 100\n",
    "    if entry_wrong_number_registration_id.empty: \n",
    "        if item not in wrong_number:\n",
    "            wrong_number.append(item)\n",
    "        fehlercount += 10\n",
    "    if entry_wrong_number_iban.empty: \n",
    "        if item not in wrong_number:\n",
    "            wrong_number.append(item)\n",
    "        fehlercount += 1\n",
    "    \n",
    "    df_scorecard_dataframe.loc[i, 'Fehlercode'] = fehlercount\n",
    "    df_scorecard_dataframe.loc[i, 'DocumentID'] = wrong_documentids\n",
    "    df_scorecard_dataframe.loc[i, 'MissingCode'] = error_codes\n",
    "\n",
    "    wrong_number.append(fehlercount)\n",
    "    if wrong_documentids:\n",
    "        wrong_number.append(wrong_documentids)\n",
    "\n",
    "    i+=1\n",
    "    fehlercount = 0\n",
    "\n",
    "print(wrong_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard_dataframe.to_json('data/scorecard_df.json', orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_debitor[\"Mandant\"] = df_results_debitor.index\n",
    "df_results_debitor.to_json('data/results_debitor.json', orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_data(df):\n",
    "    documentid_list = df[['DocumentID']].values.tolist()\n",
    "    unique_data = [list(x) for x in set(tuple(x) for x in documentid_list)]\n",
    "    res = list(map(''.join, unique_data))\n",
    "    len_unique_docids = len(unique_data)\n",
    "    for entry in res:\n",
    "        condition = df['DocumentID'] == entry\n",
    "        result = df.loc[condition]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997        True\n",
      "981       False\n",
      "983       False\n",
      "984       False\n",
      "985       False\n",
      "          ...  \n",
      "108149    False\n",
      "108150    False\n",
      "108151    False\n",
      "108135    False\n",
      "108141    False\n",
      "Name: DocumentID, Length: 224159, dtype: bool\n",
      "2825 6235\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/cclogattributes_T'+tenant+'_reduced.csv', encoding='utf-8')\n",
    "df_sorted = df.sort_values(by='DocumentID')\n",
    "# Finde die Indizes, an denen sich der Eintrag in \"Dokument\" ändert\n",
    "document_changes = df_sorted['DocumentID'].ne(df_sorted['DocumentID'].shift())\n",
    "previous_document = None\n",
    "bad_documents = 0\n",
    "whole_documents = 0\n",
    "for index, row in df_sorted.iterrows():\n",
    "    count = 0\n",
    "    if document_changes.iloc[index]:\n",
    "        whole_documents += 1\n",
    "        previous_document = row['DocumentID']\n",
    "    if row['Attribute_Name'] == 'VatAmount1' and row['Delta'] == True:\n",
    "        count += 1\n",
    "    if row['Attribute_Name'] == 'NetAmount1' and row['Delta'] == True:\n",
    "        count += 1\n",
    "    if row['Attribute_Name'] == 'VatRate1' and row['Delta'] == True:\n",
    "        count += 1\n",
    "    if row['Attribute_Name'] == 'InvoiceNumber' and row['Delta'] == True:\n",
    "        count += 1\n",
    "    if row['Attribute_Name'] == 'InvoiceDate' and row['Delta'] == True:\n",
    "        count += 1\n",
    "    if(count>=1):\n",
    "        bad_documents += 1\n",
    "\n",
    "print(bad_documents,whole_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VatAmount1    1231\n",
      "NetAmount1     692\n",
      "VatRate1       365\n",
      "Name: Attribute_Name, dtype: int64\n",
      "VatAmount1                          657\n",
      "NetAmount1, VatAmount1, VatRate1    359\n",
      "NetAmount1, VatAmount1              212\n",
      "NetAmount1                          119\n",
      "VatAmount1, VatRate1                  3\n",
      "NetAmount1, VatRate1                  2\n",
      "VatRate1                              1\n",
      "Name: Attribute_Name, dtype: int64\n",
      "Häufigkeit: 0.2041797976447172 %\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/cclogattributes_T000ht3_reduced.csv', encoding='utf-8')\n",
    "\n",
    "df_sorted_without_vendor_bad = df[~((df['Attribute_Name'] == 'VENDOR_NUM') & (df['Delta'] == True))]\n",
    "df_sorted_without_vendor_bad = df_sorted_without_vendor_bad[df_sorted_without_vendor_bad['DocumentID'].isin(df_sorted_without_vendor_bad[df_sorted_without_vendor_bad['Attribute_Name'] == 'VENDOR_NUM']['DocumentID'])]\n",
    "attribute_names = ['InvoiceDate','InvoiceNumber','GrossAmount']\n",
    "attribute_names_new = ['VatRate1','NetAmount1','VatAmount1']\n",
    "filtered_df = df_sorted_without_vendor_bad[(df_sorted_without_vendor_bad['Attribute_Name'].isin(attribute_names_new)) & (df_sorted_without_vendor_bad['Delta'] == True)]\n",
    "grouped_attributes = filtered_df.groupby('DocumentID')['Attribute_Name'].agg(lambda x: ', '.join(sorted(set(x))))\n",
    "combination_counts = grouped_attributes.value_counts()\n",
    "x = filtered_df['Attribute_Name'].value_counts()\n",
    "print(x)\n",
    "print(combination_counts)\n",
    "\n",
    "grouped_df = filtered_df.groupby(df_sorted_without_vendor_bad['DocumentID'].ne(df_sorted_without_vendor_bad['DocumentID'].shift()).cumsum())\n",
    "count_per_group = grouped_df.size()\n",
    "bad_documents = len(count_per_group)\n",
    "whole_documents = df_sorted_without_vendor_bad['DocumentID'].nunique()\n",
    "value = (x[0]/whole_documents)\n",
    "good_documents = whole_documents-bad_documents\n",
    "\n",
    "print('Häufigkeit: '+str(value)+' %')\n",
    "if(value <= 0.01):\n",
    "    print('Smartinvoice Fehler')\n",
    "#print(good_documents, whole_documents, x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cclogattributes_T'+tenant+'_reduced.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1644, 6235)\n"
     ]
    }
   ],
   "source": [
    "import funci\n",
    "funci.testi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420.00000000000006\n",
      "6197\n",
      "760.9999999999999\n",
      "6207\n",
      "276.0000000000003\n",
      "6220\n",
      "78.0000000000003\n",
      "6231\n",
      "1290.0\n",
      "6207\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/cclogattributes_T000ht3_reduced.csv', encoding='utf-8')\n",
    "df_sorted = df.sort_values(by='DocumentID')\n",
    "\n",
    "x,y = get_single_value(df_sorted, 'VatRate1')\n",
    "x1,y1 = get_single_value(df_sorted, 'NetAmount1')\n",
    "x2,y2 = get_single_value(df_sorted, 'InvoiceNumber')\n",
    "x3,y3 = get_single_value(df_sorted, 'InvoiceDate')\n",
    "x4,y4 = get_single_value(df_sorted, 'VatAmount1')\n",
    "\n",
    "print((1-x)*y)\n",
    "print(y)\n",
    "print((1-x1)*y1)\n",
    "print(y1)\n",
    "print((1-x2)*y2)\n",
    "print(y2)\n",
    "print((1-x3)*y3)\n",
    "print(y3)\n",
    "print((1-x4)*y4)\n",
    "print(y4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1.000000\n",
       "1         0.999880\n",
       "2         0.999895\n",
       "3         0.999853\n",
       "4         0.999756\n",
       "            ...   \n",
       "174147    0.999872\n",
       "174148    0.999898\n",
       "174149    0.999743\n",
       "174150    0.999912\n",
       "174151    0.999801\n",
       "Length: 174152, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Attribute_Name</th>\n",
       "      <th>Attribute_Before</th>\n",
       "      <th>Attribute_After</th>\n",
       "      <th>Delta</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DocumentID</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>-0.006891</td>\n",
       "      <td>-0.007502</td>\n",
       "      <td>-0.009454</td>\n",
       "      <td>-0.009810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attribute_Name</th>\n",
       "      <td>0.000423</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011665</td>\n",
       "      <td>0.012650</td>\n",
       "      <td>0.013810</td>\n",
       "      <td>0.002291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attribute_Before</th>\n",
       "      <td>-0.006891</td>\n",
       "      <td>0.011665</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002284</td>\n",
       "      <td>-0.002284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attribute_After</th>\n",
       "      <td>-0.007502</td>\n",
       "      <td>0.012650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005262</td>\n",
       "      <td>0.002802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta</th>\n",
       "      <td>-0.009454</td>\n",
       "      <td>0.013810</td>\n",
       "      <td>-0.002284</td>\n",
       "      <td>0.005262</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.948129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <td>-0.009810</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>-0.002284</td>\n",
       "      <td>0.002802</td>\n",
       "      <td>0.948129</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  DocumentID  Attribute_Name  Attribute_Before  \\\n",
       "DocumentID          1.000000        0.000423         -0.006891   \n",
       "Attribute_Name      0.000423        1.000000          0.011665   \n",
       "Attribute_Before   -0.006891        0.011665          1.000000   \n",
       "Attribute_After    -0.007502        0.012650          1.000000   \n",
       "Delta              -0.009454        0.013810         -0.002284   \n",
       "Type               -0.009810        0.002291         -0.002284   \n",
       "\n",
       "                  Attribute_After     Delta      Type  \n",
       "DocumentID              -0.007502 -0.009454 -0.009810  \n",
       "Attribute_Name           0.012650  0.013810  0.002291  \n",
       "Attribute_Before         1.000000 -0.002284 -0.002284  \n",
       "Attribute_After          1.000000  0.005262  0.002802  \n",
       "Delta                    0.005262  1.000000  0.948129  \n",
       "Type                     0.002802  0.948129  1.000000  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def handle_non_numerical_data_correlation(df):\n",
    "    columns = df.columns.values\n",
    "    for column in columns:\n",
    "        text_digit_vals = {}\n",
    "        def convert_to_int(val):\n",
    "            return text_digit_vals[val]\n",
    "\n",
    "        if df[column].dtype != np.int64 and df[column].dtype != np.float64:\n",
    "            column_contents = df[column].values.tolist()\n",
    "            unique_elements = set(column_contents)\n",
    "            x = 0\n",
    "            for unique in unique_elements:\n",
    "                if unique not in text_digit_vals:\n",
    "                    text_digit_vals[unique] = x\n",
    "                    x+=1\n",
    "\n",
    "            df[column] = list(map(convert_to_int, df[column]))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "bc = handle_non_numerical_data_correlation(df_table_cclogattributes)\n",
    "#display(bc)\n",
    "display(bc.corrwith(bc.iloc[0], axis=1))\n",
    "\n",
    "corr = bc.corr(method = 'pearson')\n",
    "corr\n",
    "#plt.figure(figsize=(10,8), dpi =500)\n",
    "#sns.heatmap(corr,annot=True,fmt=\".2f\", linewidth=.5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gruppieren und zählen der eindeutigen Kombinationen\n",
    "# Berechnung der Korrelationswahrscheinlichkeit\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Korrelationsanalyse\n",
    "\n",
    "# Datensatz einlesen\n",
    "df = df_table_cclogattributes\n",
    "\n",
    "df[\"Attribute_Before\"] = pd.to_numeric(df[\"Attribute_Before\"], errors=\"coerce\")\n",
    "df[\"Attribute_After\"] = pd.to_numeric(df[\"Attribute_After\"], errors=\"coerce\")\n",
    "\n",
    "# Entferne Zeilen mit NaN-Werten in den relevanten Spalten\n",
    "df = df.dropna(subset=[\"Attribute_Before\", \"Attribute_After\"])\n",
    "\n",
    "# Korrelationsanalyse durchführen\n",
    "correlation_data = []\n",
    "for attribute_name in df[\"Attribute_Name\"].unique():\n",
    "    attribute_data = df[df[\"Attribute_Name\"] == attribute_name]\n",
    "    if len(attribute_data) > 1:\n",
    "        correlation_coefficient, _ = pearsonr(attribute_data[\"Attribute_Before\"], attribute_data[\"Attribute_After\"])\n",
    "        correlation_data.append({\n",
    "            \"Attribute_Name\": attribute_name,\n",
    "            \"Correlation_Coefficient\": correlation_coefficient\n",
    "        })\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "correlation_df = pd.DataFrame(correlation_data)\n",
    "print(correlation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def cramers_v(confusion_matrix):\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "    rcorr = r - ((r - 1) ** 2) / (n - 1)\n",
    "    kcorr = k - ((k - 1) ** 2) / (n - 1)\n",
    "    return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))\n",
    "\n",
    "# Korrelationsanalyse mit Cramér's V-Koeffizient\n",
    "correlation_data = []\n",
    "for i in range(len(unique_entries)):\n",
    "    entry_1 = unique_entries[i]\n",
    "    entry_row_1 = df_table_cclogattributes[df_table_cclogattributes[\"Attribute_Name\"] == entry_1]\n",
    "    \n",
    "    for j in range(i+1, len(unique_entries)):\n",
    "        entry_2 = unique_entries[j]\n",
    "        entry_row_2 = df_table_cclogattributes[df_table_cclogattributes[\"Attribute_Name\"] == entry_2]\n",
    "        \n",
    "        if len(entry_row_1) > 0 and len(entry_row_2) > 0:\n",
    "            # Kreuztabelle erstellen\n",
    "            contingency_table = pd.crosstab(entry_row_1[\"Attribute_Before\"], entry_row_2[\"Attribute_Before\"])\n",
    "            \n",
    "            # Cramér's V-Koeffizient berechnen\n",
    "            correlation_coefficient = cramers_v(contingency_table.values)\n",
    "            \n",
    "            correlation_data.append({\n",
    "                \"Attribute_Name_1\": entry_1,\n",
    "                \"Attribute_Name_2\": entry_2,\n",
    "                \"Correlation_Coefficient\": correlation_coefficient\n",
    "            })\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "correlation_df = pd.DataFrame(correlation_data)\n",
    "print(correlation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_attributes = [\"Attribute_Name\", \"Attribute_After\"]\n",
    "target_df = bc[bc[\"Attribute_Name\"].isin(target_attributes)]\n",
    "\n",
    "# Berechnen Sie die Korrelationen der Zielzeilen mit den anderen Zeilen in \"Attribute After\"\n",
    "corr_with_target = target_df[\"Attribute_After\"].corrwith(bc[\"Attribute_After\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/cclogattributes_T0012hb.csv', encoding='iso-8859-15')\n",
    "\n",
    "vergleiche_spalten = lambda x: 'master_data_error' if x['Attribute_Name'] == 'VENDOR_NUM' and x['Attribute_Before'] != x['Attribute_After'] else'good' if x['Delta'] == False and x['Type'] == 0 else 'undefined'\n",
    "#else 'master_data_error' if x['Attribute_Name'] == 'VENDOR_NAME' and x['Attribute_Before'] != x['Attribute_After'] \\\n",
    "#else 'master_data_error' if x['Attribute_Name'] == 'VENDOR_STR' and x['Attribute_Before'] != x['Attribute_After'] \\\n",
    "#else 'master_data_error' if x['Attribute_Name'] == 'VENDOR_CITY' and x['Attribute_Before'] != x['Attribute_After'] \\\n",
    "#else 'master_data_error' if x['Attribute_Name'] == 'VENDOR_ZIP_CODE' and x['Attribute_Before'] != x['Attribute_After'] \\\n",
    "\n",
    "df1['Suggestion'] = df1.apply(vergleiche_spalten, axis=1)\n",
    "\n",
    "df1 = df1.drop('LogTimeTicks', axis=1)\n",
    "#df = df.drop('DocumentID', axis=1)\n",
    "\n",
    "df1.to_csv('data/after_preprocessing.csv', index=False, header= True, encoding='iso-8859-15')\n",
    "\n",
    "X = df1.loc[ : , df1.columns != 'Suggestion']\n",
    "Y = df1.loc[ : , df1.columns == 'Suggestion']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_non_numerical_data(df):\n",
    "    columns = df.columns.values\n",
    "    for column in columns:\n",
    "        text_digit_vals = {}\n",
    "        def convert_to_int(val):\n",
    "            return text_digit_vals[val]\n",
    "\n",
    "        if df[column].dtype != np.int64 and df[column].dtype != np.float64:\n",
    "            column_contents = df[column].values.tolist()\n",
    "            unique_elements = set(column_contents)\n",
    "            x = 0\n",
    "            for unique in unique_elements:\n",
    "                if unique not in text_digit_vals:\n",
    "                    text_digit_vals[unique] = x\n",
    "                    x+=1\n",
    "\n",
    "            df[column] = list(map(convert_to_int, df[column]))\n",
    "\n",
    "    return df\n",
    "\n",
    "#X = handle_non_numerical_data(X)\n",
    "\n",
    "\n",
    "def Encoder(df):\n",
    "          columnsToEncode = list(df.select_dtypes(include=['category','object','bool']))\n",
    "          le = LabelEncoder()\n",
    "          for feature in columnsToEncode:\n",
    "              try:\n",
    "                  df[feature] = le.fit_transform(df[feature])\n",
    "              except:\n",
    "                  print('Error encoding '+feature)\n",
    "          return df\n",
    "\n",
    "\n",
    "#Y = Encoder(Y)\n",
    "#print(Y)\n",
    "#pd.DataFrame(Y).to_csv(\"data/after_numerical.csv\")\n",
    "\n",
    "\n",
    "#print(X)\n",
    "#X.shape[0]\n",
    "#Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training and testing samples\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=None, shuffle=False)\n",
    "\n",
    "pd.DataFrame(y_train).to_csv(\"data/y_train.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam', init='uniform'):\n",
    "    # create model\n",
    "    if verbose: print(\"**Create model with optimizer: %s; init: %s\" % (optimizer, init) )\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=X_train.shape[1], kernel_initializer=init, activation='relu')) #\n",
    "    model.add(Dense(512, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(256, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(3, kernel_initializer=init, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adadelta(), metrics=['acc']) #keras.optimizers.Adadelta() and sparse_categorical_accuracy\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_gridsearch = False\n",
    "\n",
    "if run_gridsearch:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    if verbose: print (time.strftime( \"%H:%M:%S \" + \"GridSearch started ... \" ) )\n",
    "    optimizers = ['rmsprop'] # , 'adam'\n",
    "    inits = ['glorot_uniform'] #, 'normal', 'uniform'\n",
    "    epochs = [5, 50, 100, 300]\n",
    "    batches = [5, 32, 64]\n",
    "    \n",
    "    model = KerasClassifier(build_fn=create_model, verbose=verbose)\n",
    "    \n",
    "    param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    if verbose: \n",
    "        for mean, stdev, param in zip(means, stds, params):\n",
    "            print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "        elapsed_time = time.time() - start_time  \n",
    "        print (\"Time elapsed: \",timedelta(seconds=elapsed_time))\n",
    "        \n",
    "    best_epochs = grid_result.best_params_['epochs']\n",
    "    best_batch_size = grid_result.best_params_['batch_size']\n",
    "    best_init = grid_result.best_params_['init']\n",
    "    best_optimizer = grid_result.best_params_['optimizer']\n",
    "    \n",
    "else:\n",
    "    # best paramters\n",
    "    best_epochs = 300\n",
    "    best_batch_size = 64\n",
    "    best_init = 'glorot_uniform'\n",
    "    best_optimizer = 'rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(grid_result.cv_results_)\n",
    "pd.DataFrame(results_df).to_csv('data/grid_search_results.csv')\n",
    "results_df = results_df.sort_values(by=[\"rank_test_score\"])\n",
    "results_df = results_df.set_index(\n",
    "    results_df[\"params\"].apply(lambda x: \"_\".join(str(val) for val in x.values()))\n",
    ").rename_axis(\"kernel\")\n",
    "results_df[[\"params\", \"rank_test_score\", \"mean_test_score\", \"std_test_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = results_df.filter(regex=r\"split\\d*_test_score\")\n",
    "\n",
    "# plot 30 examples of dependency between cv fold and AUC scores\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(\n",
    "    data=model_scores.transpose().iloc[:30],\n",
    "    dashes=False,\n",
    "    palette=\"Set1\",\n",
    "    marker=\"o\",\n",
    "    alpha=0.5,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_xlabel(\"CV test fold\", size=12, labelpad=10)\n",
    "ax.set_ylabel(\"Model AUC\", size=12)\n",
    "ax.tick_params(bottom=True, labelbottom=False)\n",
    "ax.legend(loc=4, prop={'size': 6})\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred = KerasClassifier(build_fn=create_model, optimizer=best_optimizer, init=best_init, epochs=best_epochs, batch_size=best_batch_size, verbose=verbose)\n",
    "model_pred.fit(X_train, y_train)\n",
    "\n",
    "prediction = model_pred.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, prediction, zero_division=False))\n",
    "accuracy_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction)\n",
    "pd.DataFrame(prediction).to_csv('data/prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_pred, open('models/classification_model.h5', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/classification_model.h5', 'rb') as pickle_file:\n",
    "    content = pickle.load(pickle_file)\n",
    "\n",
    "prediction = content.predict(X_test)\n",
    "print(classification_report(y_test, prediction, zero_division=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import time\n",
    "while True:\n",
    "    for i in range(0,100):\n",
    "        pyautogui.moveTo(10,10)\n",
    "        pyautogui.leftClick()\n",
    "        pyautogui.moveTo(1000,1000)\n",
    "        pyautogui.leftClick()\n",
    "\n",
    "        time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59d736765ce23fba7eae8e4973aae525338b5974b741b35ab5c702ed8c74548c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
